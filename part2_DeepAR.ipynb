{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Workflow\n",
    "1. Loading the data\n",
    "2. Creating training and test sets of time series\n",
    "3. Formatting data as JSON files and uploading to S3\n",
    "4. Instantiating and training a DeepAR estimator\n",
    "5. Apply the estimator with Batch Transform on the input data\n",
    "6. Evaluating the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>ID</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>Promo_2_active</th>\n",
       "      <th>Open_sunday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>5263</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1270</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>6064</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>570</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>8314</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>13995</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>620</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>4822</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29910</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Store  DayOfWeek        Date  Sales  Open  Promo  StateHoliday  \\\n",
       "0           0      1          5  2015-07-31   5263     1      1             1   \n",
       "1           1      2          5  2015-07-31   6064     1      1             1   \n",
       "2           2      3          5  2015-07-31   8314     1      1             1   \n",
       "3           3      4          5  2015-07-31  13995     1      1             1   \n",
       "4           4      5          5  2015-07-31   4822     1      1             1   \n",
       "\n",
       "   SchoolHoliday  ID  StoreType  Assortment  CompetitionDistance  \\\n",
       "0              1   0          2           0                 1270   \n",
       "1              1   1          0           0                  570   \n",
       "2              1   2          0           0                14130   \n",
       "3              1   3          2           2                  620   \n",
       "4              1   4          0           0                29910   \n",
       "\n",
       "   Promo_2_active  Open_sunday  \n",
       "0               0            0  \n",
       "1               1            0  \n",
       "2               1            0  \n",
       "3               0            0  \n",
       "4               0            0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "df = pd.read_csv('prepared_data_all.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change order of df so latest date comes first\n",
    "df = df.iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>ID</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>Promo_2_active</th>\n",
       "      <th>Open_sunday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1017208</th>\n",
       "      <td>1114</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1017208</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5350</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017207</th>\n",
       "      <td>1113</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1017207</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>870</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017206</th>\n",
       "      <td>1112</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1017206</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017205</th>\n",
       "      <td>1111</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1017205</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017204</th>\n",
       "      <td>1110</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1017204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Store  DayOfWeek        Date  Sales  Open  Promo  StateHoliday  \\\n",
       "1017208   1114          2  2013-01-01      0     0      0             1   \n",
       "1017207   1113          2  2013-01-01      0     0      0             1   \n",
       "1017206   1112          2  2013-01-01      0     0      0             1   \n",
       "1017205   1111          2  2013-01-01      0     0      0             1   \n",
       "1017204   1110          2  2013-01-01      0     0      0             1   \n",
       "\n",
       "         SchoolHoliday       ID  StoreType  Assortment  CompetitionDistance  \\\n",
       "1017208              1  1017208          3           2                 5350   \n",
       "1017207              1  1017207          0           2                  870   \n",
       "1017206              1  1017206          0           2                 9260   \n",
       "1017205              1  1017205          2           2                 1880   \n",
       "1017204              1  1017204          0           0                 1900   \n",
       "\n",
       "         Promo_2_active  Open_sunday  \n",
       "1017208               0            0  \n",
       "1017207               0            0  \n",
       "1017206               0            0  \n",
       "1017205               0            0  \n",
       "1017204               0            0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the DeepAr Algorithm, Categorical features must be encoded as a 0-based sequence of positive integers\n",
    "# thus the stores (which is the category I want to distinguish in this time series) needs to start from zero and not from 1\n",
    "\n",
    "# reduce all store numbers by one\n",
    "df ['Store'] =  df.Store.apply(lambda x: (x-1))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter into train/test before running transformation\n",
    "\n",
    "# as defined in Proposal: \n",
    "# train range 07.01.2013 â€“ 07.06.2015 (94,7% of data)\n",
    "# test range 08.06.2015-26.07.2015 (5,3% of data) \n",
    "\n",
    "# TRAIN\n",
    "# Filter out all rows with a date past 07.06.2015\n",
    "df_train = df[df['Date']<'2015-06-08']\n",
    "# Filter out all rows with a date before 07.01.2013\n",
    "df_train = df_train[df_train['Date']>='2013-07-01']\n",
    "\n",
    "# TEST\n",
    "# !!!!! for this algorithm, the test set contains the complete range of each time series.!!!!!\n",
    "# Filter out all rows with a date before 08.06.2015\n",
    "df_test = df[df['Date']>='2013-07-01']\n",
    "# Filter out all rows with a date past 26.07.2015\n",
    "df_test = df_test[df_test['Date']<'2015-07-27']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Date'] = pd.to_datetime(df_train.Date)\n",
    "df_test['Date'] = pd.to_datetime(df_test.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.set_index('Date')\n",
    "df_test = df_test.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to JASON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset a list of Stores to iterate over\n",
    "store_nr = list(df_train['Store'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json for formatting data and os for saving\n",
    "import json\n",
    "import os \n",
    "\n",
    "# transforming df\n",
    "\n",
    "def write_json_dataset(df, filename): \n",
    "    with open(filename, 'wb') as f:\n",
    "        # for each of our times series, there is one JSON line\n",
    "        for store in store_nr:\n",
    "            df_store = df.loc[df['Store'] == store]\n",
    "            obj = {\"start\": str(df_store.index[0]), \"target\": list(df_store.Sales), \"cat\": [int(store)], \"dynamic_feat\": [list(df_store.DayOfWeek),list(df_store.Open),list(df_store.Promo),list(df_store.StateHoliday),list(df_store.SchoolHoliday),list(df_store.StoreType),list(df_store.Assortment),list(df_store.CompetitionDistance),list(df_store.Promo_2_active),list(df_store.Open_sunday)]}\n",
    "            json_line = json.dumps(obj) + '\\n'\n",
    "            json_line = json_line.encode('utf-8')\n",
    "            f.write(json_line)\n",
    "    print(filename + ' saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this data to a local directory\n",
    "data_dir = 'json_rossmann'\n",
    "\n",
    "# make data dir, if it does not exist\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_rossmann/train.json saved.\n",
      "json_rossmann/test.json saved.\n"
     ]
    }
   ],
   "source": [
    "# directories to save train/test data\n",
    "train_key = os.path.join(data_dir, 'train.json')\n",
    "test_key = os.path.join(data_dir, 'test.json')\n",
    "\n",
    "# write train/test JSON files\n",
    "write_json_dataset(df_train, train_key)        \n",
    "write_json_dataset(df_test, test_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general prefix\n",
    "prefix='deepar-rossmann'\n",
    "\n",
    "# *unique* train/test prefixes\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "# uploading data to S3, and saving locations\n",
    "train_path  = sagemaker_session.upload_data(train_key, bucket=bucket, key_prefix=train_prefix)\n",
    "test_path   = sagemaker_session.upload_data(test_key,  bucket=bucket, key_prefix=test_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is stored in: s3://sagemaker-eu-central-1-395339144106/deepar-rossmann/train/train.json\n",
      "Test data is stored in: s3://sagemaker-eu-central-1-395339144106/deepar-rossmann/test/test.json\n"
     ]
    }
   ],
   "source": [
    "# check locations\n",
    "print('Training data is stored in: '+ train_path)\n",
    "print('Test data is stored in: '+ test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking a preview of the json the data at s3 through the aws console shows, that all the data is in the right format as indicated in the aws documentation: https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling DeepAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "image_name = get_image_uri(boto3.Session().region_name, # get the region\n",
    "                           'forecasting-deepar') # specify image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# dir to save model artifacts\n",
    "s3_output_path = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "\n",
    "# instantiate a DeepAR estimator\n",
    "estimator = Estimator(sagemaker_session=sagemaker_session,\n",
    "                      image_name=image_name,\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.p2.xlarge',\n",
    "                      output_path=s3_output_path\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq='D'\n",
    "prediction_length= 49 # number of days in test data set (7 weeks x 7 days)\n",
    "context_length= 490 # less then number of days in train data set (126 weeks x 7 days); \n",
    "# \"a model can look further back in the time series than the value specified for context_length\"\n",
    "epochs = 50 # the maximum number of times to pass over the data when training\n",
    "# Further parameter explenation: https://docs.aws.amazon.com/forecast/latest/dg/aws-forecast-recipe-deeparplus.html \n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": str(epochs),\n",
    "    \"time_freq\": freq,\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"context_length\": str(context_length),\n",
    "    \"num_cells\": \"50\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"mini_batch_size\": \"128\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparams\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-09 10:17:38 Starting - Starting the training job...\n",
      "2020-04-09 10:17:40 Starting - Launching requested ML instances......\n",
      "2020-04-09 10:19:05 Starting - Preparing the instances for training......\n",
      "2020-04-09 10:19:59 Downloading - Downloading input data...\n",
      "2020-04-09 10:20:10 Training - Downloading the training image...\n",
      "2020-04-09 10:21:00 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:01 INFO 140402030520128] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:01 INFO 140402030520128] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.001', u'num_cells': u'50', u'prediction_length': u'49', u'epochs': u'50', u'time_freq': u'D', u'context_length': u'490', u'num_layers': u'3', u'mini_batch_size': u'128', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:01 INFO 140402030520128] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_layers': u'3', u'epochs': u'50', u'embedding_dimension': u'10', u'num_cells': u'50', u'_num_kv_servers': u'auto', u'mini_batch_size': u'128', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'49', u'time_freq': u'D', u'context_length': u'490', u'_kvstore': u'auto', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:01 INFO 140402030520128] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:01 INFO 140402030520128] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:01 INFO 140402030520128] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:01 INFO 140402030520128] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:02 INFO 140402030520128] [cardinality=auto] Inferred value of cardinality=[1115] from dataset.\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:02 INFO 140402030520128] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=10 from dataset.\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:02 INFO 140402030520128] Training set statistics:\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:02 INFO 140402030520128] Integer time series\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:02 INFO 140402030520128] number of time series: 1115\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:02 INFO 140402030520128] number of observations: 755185\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:02 INFO 140402030520128] mean target length: 677\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:02 INFO 140402030520128] min/mean/max target: 0.0/5829.70222528/38722.0\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:02 INFO 140402030520128] mean abs(target): 5829.70222528\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:02 INFO 140402030520128] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:02 INFO 140402030520128] Small number of time series. Doing 2 passes over dataset with prob 0.57399103139 per epoch.\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:03 INFO 140402030520128] Test set statistics:\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:03 INFO 140402030520128] Integer time series\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:03 INFO 140402030520128] number of time series: 1115\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:03 INFO 140402030520128] number of observations: 809820\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:03 INFO 140402030520128] mean target length: 726\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:03 INFO 140402030520128] min/mean/max target: 0.0/5832.84927885/41551.0\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:03 INFO 140402030520128] mean abs(target): 5832.84927885\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:03 INFO 140402030520128] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:03 INFO 140402030520128] nvidia-smi took: 0.075455904007 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:03 INFO 140402030520128] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:03 INFO 140402030520128] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 10393.289804458618, \"sum\": 10393.289804458618, \"min\": 10393.289804458618}}, \"EndTime\": 1586427673.998246, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427663.604034}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:13 INFO 140402030520128] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 16332.832098007202, \"sum\": 16332.832098007202, \"min\": 16332.832098007202}}, \"EndTime\": 1586427679.937011, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427673.998338}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:25 INFO 140402030520128] Epoch[0] Batch[0] avg_epoch_loss=7.384654\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:25 INFO 140402030520128] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=7.38465356827\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:37 INFO 140402030520128] Epoch[0] Batch[5] avg_epoch_loss=6.505660\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:37 INFO 140402030520128] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=6.50565989812\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:37 INFO 140402030520128] Epoch[0] Batch [5]#011Speed: 51.05 samples/sec#011loss=6.505660\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:47 INFO 140402030520128] processed a total of 1181 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}, \"update.time\": {\"count\": 1, \"max\": 27167.601108551025, \"sum\": 27167.601108551025, \"min\": 27167.601108551025}}, \"EndTime\": 1586427707.10481, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427679.937111}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:47 INFO 140402030520128] #throughput_metric: host=algo-1, train throughput=43.4706974837 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:47 INFO 140402030520128] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:47 INFO 140402030520128] #quality_metric: host=algo-1, epoch=0, train loss <loss>=6.6956533432\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:47 INFO 140402030520128] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:47 INFO 140402030520128] Saved checkpoint to \"/opt/ml/model/state_20b609ff-188c-42da-ab1a-ea65549b7629-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 266.8139934539795, \"sum\": 266.8139934539795, \"min\": 266.8139934539795}}, \"EndTime\": 1586427707.372349, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427707.104892}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:50 INFO 140402030520128] Epoch[1] Batch[0] avg_epoch_loss=6.452185\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:21:50 INFO 140402030520128] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=6.45218467712\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:03 INFO 140402030520128] Epoch[1] Batch[5] avg_epoch_loss=6.208220\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:03 INFO 140402030520128] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=6.20822016398\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:03 INFO 140402030520128] Epoch[1] Batch [5]#011Speed: 50.89 samples/sec#011loss=6.208220\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:15 INFO 140402030520128] Epoch[1] Batch[10] avg_epoch_loss=5.702311\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:15 INFO 140402030520128] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=5.09522066116\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:15 INFO 140402030520128] Epoch[1] Batch [10]#011Speed: 54.31 samples/sec#011loss=5.095221\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:15 INFO 140402030520128] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27783.81896018982, \"sum\": 27783.81896018982, \"min\": 27783.81896018982}}, \"EndTime\": 1586427735.156319, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427707.372431}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:15 INFO 140402030520128] #throughput_metric: host=algo-1, train throughput=46.5016871301 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:15 INFO 140402030520128] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:15 INFO 140402030520128] #quality_metric: host=algo-1, epoch=1, train loss <loss>=5.70231129906\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:15 INFO 140402030520128] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:15 INFO 140402030520128] Saved checkpoint to \"/opt/ml/model/state_6113ab23-1998-458b-b8e9-18450178dcee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 270.8301544189453, \"sum\": 270.8301544189453, \"min\": 270.8301544189453}}, \"EndTime\": 1586427735.427756, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427735.156395}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:18 INFO 140402030520128] Epoch[2] Batch[0] avg_epoch_loss=6.263884\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:18 INFO 140402030520128] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=6.2638835907\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/09/2020 10:22:31 INFO 140402030520128] Epoch[2] Batch[5] avg_epoch_loss=6.071476\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:31 INFO 140402030520128] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=6.0714764595\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:31 INFO 140402030520128] Epoch[2] Batch [5]#011Speed: 51.02 samples/sec#011loss=6.071476\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:43 INFO 140402030520128] Epoch[2] Batch[10] avg_epoch_loss=5.752288\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:43 INFO 140402030520128] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=5.36926279068\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:43 INFO 140402030520128] Epoch[2] Batch [10]#011Speed: 54.37 samples/sec#011loss=5.369263\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:43 INFO 140402030520128] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27725.69489479065, \"sum\": 27725.69489479065, \"min\": 27725.69489479065}}, \"EndTime\": 1586427763.153605, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427735.427842}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:43 INFO 140402030520128] #throughput_metric: host=algo-1, train throughput=46.5991558814 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:43 INFO 140402030520128] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:43 INFO 140402030520128] #quality_metric: host=algo-1, epoch=2, train loss <loss>=5.75228842822\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:43 INFO 140402030520128] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:46 INFO 140402030520128] Epoch[3] Batch[0] avg_epoch_loss=6.241644\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:46 INFO 140402030520128] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=6.24164390564\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:59 INFO 140402030520128] Epoch[3] Batch[5] avg_epoch_loss=6.202085\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:59 INFO 140402030520128] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=6.20208501816\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:22:59 INFO 140402030520128] Epoch[3] Batch [5]#011Speed: 51.09 samples/sec#011loss=6.202085\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:08 INFO 140402030520128] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25354.047060012817, \"sum\": 25354.047060012817, \"min\": 25354.047060012817}}, \"EndTime\": 1586427788.508238, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427763.153694}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:08 INFO 140402030520128] #throughput_metric: host=algo-1, train throughput=50.1298404865 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:08 INFO 140402030520128] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:08 INFO 140402030520128] #quality_metric: host=algo-1, epoch=3, train loss <loss>=6.12275824547\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:08 INFO 140402030520128] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:11 INFO 140402030520128] Epoch[4] Batch[0] avg_epoch_loss=5.966991\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:11 INFO 140402030520128] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=5.96699094772\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:24 INFO 140402030520128] Epoch[4] Batch[5] avg_epoch_loss=6.072946\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:24 INFO 140402030520128] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=6.07294623057\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:24 INFO 140402030520128] Epoch[4] Batch [5]#011Speed: 51.03 samples/sec#011loss=6.072946\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:36 INFO 140402030520128] Epoch[4] Batch[10] avg_epoch_loss=5.942448\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:36 INFO 140402030520128] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=5.78584957123\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:36 INFO 140402030520128] Epoch[4] Batch [10]#011Speed: 54.31 samples/sec#011loss=5.785850\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:36 INFO 140402030520128] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27741.070985794067, \"sum\": 27741.070985794067, \"min\": 27741.070985794067}}, \"EndTime\": 1586427816.249897, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427788.508311}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:36 INFO 140402030520128] #throughput_metric: host=algo-1, train throughput=46.5012612484 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:36 INFO 140402030520128] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:36 INFO 140402030520128] #quality_metric: host=algo-1, epoch=4, train loss <loss>=5.94244774905\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:36 INFO 140402030520128] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:39 INFO 140402030520128] Epoch[5] Batch[0] avg_epoch_loss=6.079942\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:39 INFO 140402030520128] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=6.07994174957\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:52 INFO 140402030520128] Epoch[5] Batch[5] avg_epoch_loss=6.197651\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:52 INFO 140402030520128] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=6.19765090942\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:23:52 INFO 140402030520128] Epoch[5] Batch [5]#011Speed: 51.00 samples/sec#011loss=6.197651\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:01 INFO 140402030520128] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25425.805807113647, \"sum\": 25425.805807113647, \"min\": 25425.805807113647}}, \"EndTime\": 1586427841.676244, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427816.249976}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:01 INFO 140402030520128] #throughput_metric: host=algo-1, train throughput=49.5950272582 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:01 INFO 140402030520128] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:01 INFO 140402030520128] #quality_metric: host=algo-1, epoch=5, train loss <loss>=6.11045212746\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:01 INFO 140402030520128] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:05 INFO 140402030520128] Epoch[6] Batch[0] avg_epoch_loss=6.373661\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:05 INFO 140402030520128] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=6.37366104126\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:17 INFO 140402030520128] Epoch[6] Batch[5] avg_epoch_loss=6.002810\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:17 INFO 140402030520128] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=6.00280968348\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:17 INFO 140402030520128] Epoch[6] Batch [5]#011Speed: 50.98 samples/sec#011loss=6.002810\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:29 INFO 140402030520128] Epoch[6] Batch[10] avg_epoch_loss=5.737436\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:29 INFO 140402030520128] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=5.41898803711\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:29 INFO 140402030520128] Epoch[6] Batch [10]#011Speed: 54.26 samples/sec#011loss=5.418988\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:29 INFO 140402030520128] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27773.38409423828, \"sum\": 27773.38409423828, \"min\": 27773.38409423828}}, \"EndTime\": 1586427869.450231, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427841.676333}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:29 INFO 140402030520128] #throughput_metric: host=algo-1, train throughput=47.0952680899 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:29 INFO 140402030520128] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:29 INFO 140402030520128] #quality_metric: host=algo-1, epoch=6, train loss <loss>=5.73743620786\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:29 INFO 140402030520128] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:32 INFO 140402030520128] Epoch[7] Batch[0] avg_epoch_loss=5.625690\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:32 INFO 140402030520128] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=5.62568950653\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:45 INFO 140402030520128] Epoch[7] Batch[5] avg_epoch_loss=5.979083\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:45 INFO 140402030520128] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=5.97908290227\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:45 INFO 140402030520128] Epoch[7] Batch [5]#011Speed: 51.15 samples/sec#011loss=5.979083\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[04/09/2020 10:24:57 INFO 140402030520128] Epoch[7] Batch[10] avg_epoch_loss=6.237179\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:57 INFO 140402030520128] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=6.54689407349\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:57 INFO 140402030520128] Epoch[7] Batch [10]#011Speed: 54.41 samples/sec#011loss=6.546894\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:57 INFO 140402030520128] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27705.15489578247, \"sum\": 27705.15489578247, \"min\": 27705.15489578247}}, \"EndTime\": 1586427897.155901, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427869.450299}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:57 INFO 140402030520128] #throughput_metric: host=algo-1, train throughput=46.3810658896 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:57 INFO 140402030520128] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:57 INFO 140402030520128] #quality_metric: host=algo-1, epoch=7, train loss <loss>=6.23717888919\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:24:57 INFO 140402030520128] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:00 INFO 140402030520128] Epoch[8] Batch[0] avg_epoch_loss=6.125992\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:00 INFO 140402030520128] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=6.12599182129\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:13 INFO 140402030520128] Epoch[8] Batch[5] avg_epoch_loss=6.102929\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:13 INFO 140402030520128] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=6.10292903582\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:13 INFO 140402030520128] Epoch[8] Batch [5]#011Speed: 51.02 samples/sec#011loss=6.102929\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:22 INFO 140402030520128] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25369.895935058594, \"sum\": 25369.895935058594, \"min\": 25369.895935058594}}, \"EndTime\": 1586427922.5264, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427897.155981}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:22 INFO 140402030520128] #throughput_metric: host=algo-1, train throughput=49.3889857734 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:22 INFO 140402030520128] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:22 INFO 140402030520128] #quality_metric: host=algo-1, epoch=8, train loss <loss>=5.99028768539\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:22 INFO 140402030520128] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:25 INFO 140402030520128] Epoch[9] Batch[0] avg_epoch_loss=6.056696\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:25 INFO 140402030520128] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=6.05669593811\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:38 INFO 140402030520128] Epoch[9] Batch[5] avg_epoch_loss=6.131433\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:38 INFO 140402030520128] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=6.13143340747\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:38 INFO 140402030520128] Epoch[9] Batch [5]#011Speed: 51.03 samples/sec#011loss=6.131433\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:47 INFO 140402030520128] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25348.926067352295, \"sum\": 25348.926067352295, \"min\": 25348.926067352295}}, \"EndTime\": 1586427947.875979, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427922.52649}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:47 INFO 140402030520128] #throughput_metric: host=algo-1, train throughput=49.6665381792 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:47 INFO 140402030520128] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:47 INFO 140402030520128] #quality_metric: host=algo-1, epoch=9, train loss <loss>=5.93343048096\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:47 INFO 140402030520128] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:51 INFO 140402030520128] Epoch[10] Batch[0] avg_epoch_loss=6.142729\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:25:51 INFO 140402030520128] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=6.14272928238\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:03 INFO 140402030520128] Epoch[10] Batch[5] avg_epoch_loss=6.147667\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:03 INFO 140402030520128] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=6.14766693115\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:03 INFO 140402030520128] Epoch[10] Batch [5]#011Speed: 50.93 samples/sec#011loss=6.147667\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:15 INFO 140402030520128] Epoch[10] Batch[10] avg_epoch_loss=5.777504\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:15 INFO 140402030520128] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=5.33330826759\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:15 INFO 140402030520128] Epoch[10] Batch [10]#011Speed: 54.29 samples/sec#011loss=5.333308\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:15 INFO 140402030520128] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 27768.33987236023, \"sum\": 27768.33987236023, \"min\": 27768.33987236023}}, \"EndTime\": 1586427975.644917, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427947.876069}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:15 INFO 140402030520128] #throughput_metric: host=algo-1, train throughput=47.0677576458 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:15 INFO 140402030520128] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:15 INFO 140402030520128] #quality_metric: host=algo-1, epoch=10, train loss <loss>=5.77750390226\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:15 INFO 140402030520128] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:19 INFO 140402030520128] Epoch[11] Batch[0] avg_epoch_loss=6.305881\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:19 INFO 140402030520128] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=6.30588054657\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:31 INFO 140402030520128] Epoch[11] Batch[5] avg_epoch_loss=5.992111\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:31 INFO 140402030520128] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=5.99211144447\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:31 INFO 140402030520128] Epoch[11] Batch [5]#011Speed: 51.01 samples/sec#011loss=5.992111\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 INFO 140402030520128] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25373.471975326538, \"sum\": 25373.471975326538, \"min\": 25373.471975326538}}, \"EndTime\": 1586428001.018993, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586427975.645011}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 INFO 140402030520128] #throughput_metric: host=algo-1, train throughput=49.6579373452 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 INFO 140402030520128] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 INFO 140402030520128] #quality_metric: host=algo-1, epoch=11, train loss <loss>=5.97722415924\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 INFO 140402030520128] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 INFO 140402030520128] Loading parameters from best epoch (1)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 122.06888198852539, \"sum\": 122.06888198852539, \"min\": 122.06888198852539}}, \"EndTime\": 1586428001.141771, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586428001.019065}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 INFO 140402030520128] stopping training now\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 INFO 140402030520128] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 INFO 140402030520128] Final loss: 5.70231129906 (occurred at epoch 1)\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 INFO 140402030520128] #quality_metric: host=algo-1, train final_loss <loss>=5.70231129906\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 WARNING 140402030520128] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 INFO 140402030520128] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 WARNING 140402030520128] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:41 INFO 140402030520128] All workers finished. Serializing model for prediction.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 10388.356924057007, \"sum\": 10388.356924057007, \"min\": 10388.356924057007}}, \"EndTime\": 1586428011.531126, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586428001.141846}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:52 INFO 140402030520128] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 11429.579973220825, \"sum\": 11429.579973220825, \"min\": 11429.579973220825}}, \"EndTime\": 1586428012.572312, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586428011.531211}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:52 INFO 140402030520128] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:52 INFO 140402030520128] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 166.12005233764648, \"sum\": 166.12005233764648, \"min\": 166.12005233764648}}, \"EndTime\": 1586428012.73856, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586428012.572387}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:52 INFO 140402030520128] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:26:52 INFO 140402030520128] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.04601478576660156, \"sum\": 0.04601478576660156, \"min\": 0.04601478576660156}}, \"EndTime\": 1586428012.739425, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586428012.738621}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 16911.10110282898, \"sum\": 16911.10110282898, \"min\": 16911.10110282898}}, \"EndTime\": 1586428029.650475, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586428012.739504}\n",
      "\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #test_score (algo-1, RMSE): 3025.24681152\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #test_score (algo-1, mean_absolute_QuantileLoss): 93651132.34275834\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #test_score (algo-1, mean_wQuantileLoss): 0.29169869969536355\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #test_score (algo-1, wQuantileLoss[0.1]): 0.23450284378361572\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #test_score (algo-1, wQuantileLoss[0.2]): 0.32024890310586507\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #test_score (algo-1, wQuantileLoss[0.3]): 0.35064480913505297\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #test_score (algo-1, wQuantileLoss[0.4]): 0.355135001803206\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #test_score (algo-1, wQuantileLoss[0.5]): 0.3464778227767722\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #test_score (algo-1, wQuantileLoss[0.6]): 0.3270554778219384\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #test_score (algo-1, wQuantileLoss[0.7]): 0.29264272828416793\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #test_score (algo-1, wQuantileLoss[0.8]): 0.2393165643571204\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #test_score (algo-1, wQuantileLoss[0.9]): 0.15926414619053328\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.291698699695\u001b[0m\n",
      "\u001b[34m[04/09/2020 10:27:09 INFO 140402030520128] #quality_metric: host=algo-1, test RMSE <loss>=3025.24681152\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 368141.78705215454, \"sum\": 368141.78705215454, \"min\": 368141.78705215454}, \"setuptime\": {\"count\": 1, \"max\": 9.40084457397461, \"sum\": 9.40084457397461, \"min\": 9.40084457397461}}, \"EndTime\": 1586428029.893181, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1586428029.650551}\n",
      "\u001b[0m\n",
      "\n",
      "2020-04-09 10:27:21 Uploading - Uploading generated training model\n",
      "2020-04-09 10:27:21 Completed - Training job completed\n",
      "Training seconds: 442\n",
      "Billable seconds: 442\n",
      "CPU times: user 1.25 s, sys: 22 ms, total: 1.27 s\n",
      "Wall time: 10min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train and test channels\n",
    "data_channels = {\n",
    "    \"train\": train_path,\n",
    "    \"test\": test_path\n",
    "}\n",
    "\n",
    "# fit the estimator\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. create predictor (by deploying estimator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!CPU times: user 250 ms, sys: 5.66 ms, total: 255 ms\n",
      "Wall time: 7min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create a predictor\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.p2.xlarge',\n",
    "    content_type=\"application/json\" # specify that it will accept/produce JSON\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. generating predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the preictor directly after transformation of the input data caused a runtime error. Therefore I changed the code to a Batch Transform Prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to change the df to a json format (similar to the actions for training the model)\n",
    "each time series (here each store) is one line of json code after transformation\n",
    "here in addition to the json_object, the configuration is also attached to each time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictor_input(df, filename): \n",
    "    with open(filename, 'wb') as f:\n",
    "        # for each of our times series, there is one JSON line\n",
    "        instances = []\n",
    "        for store in store_nr:\n",
    "            df_store = df.loc[df['Store'] == store]\n",
    "            l = list(df_store.Sales)\n",
    "            del l[-49:]\n",
    "            json_obj = {\"start\": str(df_store.index[0]), \"target\": l , \"cat\": [int(store)], \"dynamic_feat\": [list(df_store.DayOfWeek),list(df_store.Open),list(df_store.Promo),list(df_store.StateHoliday),list(df_store.SchoolHoliday),list(df_store.StoreType),list(df_store.Assortment),list(df_store.CompetitionDistance),list(df_store.Promo_2_active),list(df_store.Open_sunday)]}\n",
    "            instances.append(json_obj)\n",
    "            \n",
    "        configuration = {\"num_samples\": 10, \n",
    "                         \"output_types\": [\"mean\",\"quantiles\"], \n",
    "                         \"quantiles\": ['0.1', '0.5', '0.9']}\n",
    "        request_data = {\"instances\": instances, \n",
    "                        \"configuration\": configuration}\n",
    "        json_request = json.dumps(request_data) + '\\n'\n",
    "        json_request = json_request.encode('utf-8')\n",
    "        f.write(json_request)\n",
    "    print(filename + ' saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply function to save data in the right format for the DeppAr Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_rossmann/prediction_input.json saved.\n"
     ]
    }
   ],
   "source": [
    "# save this data to a local directory\n",
    "data_dir = 'json_rossmann'\n",
    "\n",
    "# directories to save train/test data\n",
    "input_key = os.path.join(data_dir, 'prediction_input.json')\n",
    "\n",
    "# write train/test JSON files       \n",
    "write_predictor_input(df_test, input_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saving the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction input data is stored in: s3://sagemaker-eu-central-1-395339144106/deepar-rossmann/input/prediction_input.json\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix='deepar-rossmann'\n",
    "\n",
    "# *unique* prefix\n",
    "input_prefix   = '{}/{}'.format(prefix, 'input')\n",
    "\n",
    "# uploading data to S3, and saving locations\n",
    "predicton_input_path  = sagemaker_session.upload_data(input_key, bucket=bucket, key_prefix=input_prefix)\n",
    "\n",
    "# check locations\n",
    "print('Prediction input data is stored in: '+ predicton_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Transform job (since the amount of data would cause a timeout otherwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can deploy a model in Amazon SageMaker in one of two ways:\n",
    "\n",
    "1. Create a persistent HTTPS endpoint where the model provides real-time inference.\n",
    "2. Run an Amazon SageMaker batch transform job that starts an endpoint, generates inferences on the stored dataset, outputs the inference predictions, and then shuts down the endpoint.\n",
    "\n",
    "Ressource: https://idk.dev/kinect-energy-uses-amazon-sagemaker-to-forecast-energy-prices-with-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Transform job with name:  Batch-Transform-2020-04-09-15-19-26\n",
      "Transform job ended with status: Completed\n"
     ]
    }
   ],
   "source": [
    "# Batch Transform\n",
    "\n",
    "import boto3\n",
    "# Create the SageMaker Boto3 client\n",
    "boto3_sm = boto3.client('sagemaker')\n",
    "\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "batch_job_name = 'Batch-Transform-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "input_location = 's3://sagemaker-eu-central-1-395339144106/deepar-rossmann/input/prediction_input.json'\n",
    "output_location = 's3://{}/{}/output/{}'.format(bucket, prefix, batch_job_name)\n",
    "# ModelName is copied from the finished training job at aws console\n",
    "\n",
    "request = \\\n",
    "{\n",
    "    \"BatchStrategy\": \"SingleRecord\",\n",
    "    \"MaxPayloadInMB\": 100,\n",
    "    \"TransformJobName\": batch_job_name,\n",
    "    \"ModelName\": 'forecasting-deepar-2020-04-09-10-17-38-593',\n",
    "    \"TransformOutput\": {\n",
    "        \"S3OutputPath\": output_location,\n",
    "        \"Accept\": \"application/jsonlines\",\n",
    "        \"AssembleWith\": \"Line\"\n",
    "    },\n",
    "    \"TransformInput\": {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": input_location \n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": \"application/jsonlines\",\n",
    "        \"SplitType\": \"Line\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    },\n",
    "    \"TransformResources\": {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InstanceCount\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "boto3_sm.create_transform_job(**request)\n",
    "print(\"Created Transform job with name: \", batch_job_name)\n",
    "\n",
    "# Wait until the job finishes\n",
    "try:\n",
    "    boto3_sm.get_waiter('transform_job_completed_or_stopped').wait(TransformJobName=batch_job_name)\n",
    "finally:\n",
    "    response = boto3_sm.describe_transform_job(TransformJobName=batch_job_name)\n",
    "    status = response['TransformJobStatus']\n",
    "    print(\"Transform job ended with status: \" + status) \n",
    "    if status == 'Failed':\n",
    "        message =response['FailureReason']\n",
    "        print('Transform failed with the following error: {}'.format(message))\n",
    "        raise Exception('Transform job failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inspect output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_output_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:]\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_name))\n",
    "    return obj.get()[\"Body\"].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"error\":\"'start' is a required property\\n\\nFa...</td>\n",
       "      <td>\\n     'properties': {'cat': {'anyOf': [{'type...</td>\n",
       "      <td>\\n                                      {'item...</td>\n",
       "      <td>\\n                                       'type...</td>\n",
       "      <td>\\n                    'dynamic_feat': {'items'...</td>\n",
       "      <td>\\n                                            ...</td>\n",
       "      <td>\\n                                     'type':...</td>\n",
       "      <td>\\n                    'start': {'type': 'string'}</td>\n",
       "      <td>\\n                    'target': {'items': {'an...</td>\n",
       "      <td>\\n                                            ...</td>\n",
       "      <td>\\n                                            ...</td>\n",
       "      <td>\\n                               'type': 'arra...</td>\n",
       "      <td>\\n     'required': ['start'</td>\n",
       "      <td>'target']</td>\n",
       "      <td>\\n     'type': 'object'}\\n\\nOn instance['insta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  0   \\\n",
       "0  {\"error\":\"'start' is a required property\\n\\nFa...   \n",
       "\n",
       "                                                  1   \\\n",
       "0  \\n     'properties': {'cat': {'anyOf': [{'type...   \n",
       "\n",
       "                                                  2   \\\n",
       "0  \\n                                      {'item...   \n",
       "\n",
       "                                                  3   \\\n",
       "0  \\n                                       'type...   \n",
       "\n",
       "                                                  4   \\\n",
       "0  \\n                    'dynamic_feat': {'items'...   \n",
       "\n",
       "                                                  5   \\\n",
       "0  \\n                                            ...   \n",
       "\n",
       "                                                  6   \\\n",
       "0  \\n                                     'type':...   \n",
       "\n",
       "                                                  7   \\\n",
       "0  \\n                    'start': {'type': 'string'}   \n",
       "\n",
       "                                                  8   \\\n",
       "0  \\n                    'target': {'items': {'an...   \n",
       "\n",
       "                                                  9   \\\n",
       "0  \\n                                            ...   \n",
       "\n",
       "                                                  10  \\\n",
       "0  \\n                                            ...   \n",
       "\n",
       "                                                  11  \\\n",
       "0  \\n                               'type': 'arra...   \n",
       "\n",
       "                            12          13  \\\n",
       "0  \\n     'required': ['start'   'target']   \n",
       "\n",
       "                                                  14  \n",
       "0  \\n     'type': 'object'}\\n\\nOn instance['insta...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = get_output_from_s3(output_location, '{}.out'.format('prediction_input.json'))\n",
    "#https://sagemaker-eu-central-1-395339144106.s3.eu-central-1.amazonaws.com/deepar-rossmann/output/Batch-Transform-2020-04-09-15-19-26/prediction_input.json.out\n",
    "output_df = pd.read_csv(io.StringIO(output), sep=\",\", header=None)\n",
    "output_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output looks horrible and is definetly nothing I can continue using. At this point I am stuck and will stop the DeepAr approach. It seems to me that the algorithms documentation and application is very rare when it comes to using it with multiple categories and especially features. Also for the Batch Transform application I had to spent a lot of time to figure out a way that seemed to work. I think in every project there is a time for debugging and trying different approaches and there is a time for calling the project off. There sure are other possibilites to solve this use case, but for me the goal of this udacity capstone project was not to predict sales no matter what, but to apply the aws sagemaker DeepAr Algorithm.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### not used code snippets for decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __decode_response(self, response, prediction_times, encoding=\"utf-8\"):\n",
    "    response_data = json.loads(response.decode(encoding))\n",
    "    list_of_df = []\n",
    "    for k in range(len(prediction_times)):\n",
    "        prediction_index = pd.DatetimeIndex(start=prediction_times[k], freq=self.freq, periods=self.prediction_length)\n",
    "        list_of_df.append(pd.DataFrame(data=response_data['predictions'][k]['quantiles'], index=prediction_index))\n",
    "    return list_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results \n",
    "def plot(\n",
    "    predictor, \n",
    "    target_ts, \n",
    "    cat=None, \n",
    "    dynamic_feat=None, \n",
    "    forecast_date=end_training, \n",
    "    show_samples=False, \n",
    "    plot_history=7 * 12,\n",
    "    confidence=80\n",
    "):\n",
    "    print(\"calling served model to generate predictions starting from {}\".format(str(forecast_date)))\n",
    "    assert(confidence > 50 and confidence < 100)\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": 100\n",
    "    }\n",
    "\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, 'cat = {}'.format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    if show_samples: \n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                prediction[key].plot(color='lightskyblue', alpha=0.2, label='_nolegend_')\n",
    "                \n",
    "                \n",
    "    # plot the target\n",
    "    target_section = target_ts[forecast_date-plot_history:forecast_date+prediction_length]\n",
    "    target_section.plot(color=\"black\", label='target')\n",
    "    \n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "    prediction[\"0.5\"].plot(color=\"b\", label='P50')\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "    \n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                index=pd.DatetimeIndex(start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)),\n",
    "                data=f\n",
    "            )\n",
    "            feat_ts[forecast_date-plot_history:forecast_date+prediction_length].plot(ax=ax, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
